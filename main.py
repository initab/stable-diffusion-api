import argparse
import io
import json

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from diffusers import AutoPipelineForText2Image
from fastapi.responses import StreamingResponse, JSONResponse
import torch

parser = argparse.ArgumentParser(
    description='API frontend for Stable Diffusion Models to take in parameters and return generated Image over HTTP',
    epilog='At the moment this API does not support any form of authentication nor encryption. '
           'DO NOT USE IN PRODUCTION')
parser.add_argument("-c", "--config", help="Path to config file", default="config.json", required=False)
parser.add_argument("-r", "--root", help="Root path of API", default="/api", required=False)
parser.add_argument("-e", "--endpoint", help="Endpoint name for generating image",
                    default="/generate_image", required=False)
parser.add_argument("-m", "--model", help="Which Diffusion Model to use",
                    default="segmind/SSD-1B", required=False,)
parser.add_argument("--variant", help="Which Variant model to use", default="fp16", required=False,)

args = parser.parse_args()

with open(args.config, 'r') as f:
    config = json.load(f)

# This parts makes sure that there is always a value in the config, and that arguments to the command line
# supersedes config values
for key, value in vars(args):
    if key not in config or not config[key] or parser.get_default(key) != value:
        config[key] = value


# API Start
app = FastAPI(root_path=config["root"])
# Configuration variables
repo_id = config["model"]
variant = config["variant"]


class Item(BaseModel):
    prompt: str
    inference_steps: int | None = 40
    guiding_scale: float | None = 7.5


pipeline = AutoPipelineForText2Image.from_pretrained(
    repo_id,
    variant=variant,
    torch_dtype=torch.bfloat16,
    use_safetensors=True
).to("cuda")
# Setup to be more memory efficient, so we don't run out of NVRam
pipeline.enable_vae_slicing()
pipeline.enable_vae_tiling()
pipeline.enable_attention_slicing(1)
pipeline.enable_model_cpu_offload()

gen = torch.Generator(device="cuda")


@app.post(config["endpoint"],
          # Set response template to respond with an image
          responses={
              200: {
                  "content": {"image/jpeg": {}}
              },
              500: {
                  "content": {"application/json": {}}
              }
          },
          # Make sure we don't add 'application/json' as a response when sending image. This is an image, nothing else
          response_class=StreamingResponse
          )
def generate_image(item: Item):
    """
    This function uses the FastAPI POST method to generate an image based on prompts provided by the user.

    Args:
        item (Item): An Item object that contains the following parameters:
                      prompt (str): the textual prompt for the image generation
                      inference_steps (int, optional): the number of inference steps. Defaults to 40.
                      guiding_scale (float, optional): guiding scale. Defaults to 7.5.
                      seed (int, optional): seed. Defaults to 17.

    Returns:
        response (Response): image in JPEG format, 1024x1024 pixels generated by model based on provided prompt
    """
    prompt = item.prompt
    image = pipeline(prompt,
                     num_inference_steps=item.inference_steps,
                     guidance_scale=item.guiding_scale,
                     generator=gen,
                     width=1024,
                     height=1024
                     ).images[0]

    jpeg_image = io.BytesIO()
    image.save(jpeg_image, format="JPEG")
    jpeg_image.seek(0)
    if jpeg_image.getbuffer().nbytes == 0:
        raise HTTPException(status_code=500, detail="Failed to generate image")
    return StreamingResponse(jpeg_image, media_type="image/jpeg")
