import io

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from diffusers import AutoPipelineForText2Image
from fastapi.responses import StreamingResponse, JSONResponse
import torch

app = FastAPI(root_path="/api")
# Configuration variables
repo_id = "segmind/SSD-1B"
variant = "fp16"


class Item(BaseModel):
    prompt: str
    inference_steps: int | None = 40
    guiding_scale: float | None = 7.5


pipeline = AutoPipelineForText2Image.from_pretrained(
    repo_id,
    variant=variant,
    torch_dtype=torch.bfloat16,
    use_safetensors=True
).to("cuda")
# Setup to be more memory efficient, so we don't run out of NVRam
pipeline.enable_vae_slicing()
pipeline.enable_vae_tiling()
pipeline.enable_attention_slicing(1)
pipeline.enable_model_cpu_offload()

gen = torch.Generator(device="cuda")

"""
    This function uses the FastAPI POST method to generate an image based on prompts provided by the user.
    
    Args:
        item (Item): An Item object that contains the following parameters: 
                      prompt (str): the textual prompt for the image generation
                      inference_steps (int, optional): the number of inference steps. Defaults to 40.
                      guiding_scale (float, optional): guiding scale. Defaults to 7.5.
                      seed (int, optional): seed. Defaults to 17.
                      
    Returns:
        response (Response): image in png format generated by model based on provided prompt
"""


@app.post("/generate_image",
          # Set response template to respond with an image
          responses={
              200: {
                  "content": {"image/jpeg": {}}
              },
              500: {
                  "content": {"application/json": {}}
              }
          },
          # Make sure we don't add 'application/json' as a response. This is an image, nothing else
          response_class=StreamingResponse
          )
def generate_image(item: Item):
    prompt = item.prompt
    image = pipeline(prompt,
                     num_inference_steps=item.inference_steps,
                     guidance_scale=item.guiding_scale,
                     generator=gen,
                     width=1024,
                     height=1024
                     ).images[0]

    jpeg_image = io.BytesIO()
    image.save(jpeg_image, format="JPEG")
    jpeg_image.seek(0)
    if jpeg_image.getbuffer().nbytes == 0:
        raise HTTPException(status_code=500, detail="Failed to generate image")
    return StreamingResponse(jpeg_image, media_type="image/jpeg")
